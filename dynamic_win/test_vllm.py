import json
import requests
import os
import sys
from typing import List, Dict, Any, Tuple
import numpy as np
from tqdm import tqdm
from collections import Counter  # æ·»åŠ è¿™ä¸ªå¯¼å…¥

# sys.path.append("/share/nijian")
# from FlagEmbedding import FlagReranker


class DynamicWindowProcessor:
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–åŠ¨æ€çª—å£å¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„è·¯å¾„å’Œå‚æ•°
        """
        self.config = config
        # self.reranker = FlagReranker(config["rerank_model"], devices="cuda:4")
        self.reranker = None
        
        # è¯»å–promptæ¨¡æ¿
        with open(config["prompt_file"], "r", encoding="utf-8") as f:
            self.prompt_template = f.read().strip()
        with open(config["prompt_file_refine"], "r", encoding="utf-8") as f:
            self.prompt_template_refine = f.read().strip()
    
    def _call_vllm_api(self, prompt: str, max_tokens: int = 512, temperature: float = 0.2) -> str:
        """
        è°ƒç”¨vLLM APIè¿›è¡Œæ¨ç†
        
        Args:
            prompt: è¾“å…¥çš„æç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å“åº”
        """
        url = "http://localhost:8000/v1/completions"
        payload = {
            "model": "/usr1/project/models/llama3.1-70B",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        try:
            response = requests.post(url, json=payload, headers={"Content-Type": "application/json"})
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["text"].strip()
        except Exception as e:
            print(f"APIè°ƒç”¨å¤±è´¥: {e}")
            return ""
    
    def dynamic_win_llm(self, dialogue_history: List[str], current_query: str, 
                       vote_times: int = 5, conf_threshold: float = 0.6) -> Tuple[List[int], float]:
        """
        ğŸ”¥ å¤šæ¬¡ LLM æ¨ç† + æŠ•ç¥¨é›†æˆï¼Œå‡å°‘ä¸ç¡®å®šæ€§
        
        Args:
            dialogue_history: å¯¹è¯å†å²åˆ—è¡¨
            current_query: å½“å‰æŸ¥è¯¢
            vote_times: æŠ•ç¥¨æ¬¡æ•°
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            é€‰æ‹©çš„çª—å£ç´¢å¼•åˆ—è¡¨å’Œç½®ä¿¡åº¦
        """
        formatted_history = [f"[{idx}] {turn}" for idx, turn in enumerate(dialogue_history)]
        conv_info = f"<dialogue_history>:{formatted_history}\n<current_query>:{current_query}, please find the index"
        
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = f"{self.prompt_template}\n\nUser: {conv_info}\n\nAssistant:"
        
        predictions = []
        for _ in range(vote_times):
            response = self._call_vllm_api(full_prompt, temperature=0.3)  # ç¨å¾®å¢åŠ æ¸©åº¦ä»¥è·å¾—å¤šæ ·æ€§
            try:
                result = eval(response)
                predictions.append(tuple(result))
            except Exception as e:
                print(f"è§£æLLMå“åº”å¤±è´¥: {e}")
                continue
        
        if not predictions:
            return [-1], 0.0  # å…¨éƒ¨å¤±è´¥æ—¶è¿”å›
        
        # æŠ•ç¥¨ç»Ÿè®¡
        counts = Counter(predictions)
        best_win, freq = counts.most_common(1)[0]
        confidence = freq / vote_times
        
        # åˆ¤æ–­ç½®ä¿¡åº¦
        if confidence < conf_threshold:
            print(f"âš ï¸ ä½ç½®ä¿¡åº¦ ({confidence:.2f})ï¼Œå›é€€å›ºå®šçª—å£")
            return [-1], confidence
        
        print(f"âœ… é«˜ç½®ä¿¡åº¦ ({confidence:.2f})ï¼Œé€‰æ‹©çª—å£: {best_win}")
        return list(best_win), confidence
    
    def filter_win_rerank(self, query: str, selected_win_content: List[str]) -> List[str]:
        """
        ä½¿ç”¨BGE rerankerè¿‡æ»¤çª—å£å†…å®¹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            selected_win_content: é€‰æ‹©çš„çª—å£å†…å®¹åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„çª—å£å†…å®¹åˆ—è¡¨
        """
        if not selected_win_content:
            return []
            
        pairs = [(query, dialog) for dialog in selected_win_content]
        scores = self.reranker.compute_score(pairs, normalize=True)
        
        if isinstance(scores, np.ndarray):
            scores = scores.tolist()
        
        scored_dialogs = list(zip(selected_win_content, scores))
        filtered_win = [dialog for dialog, score in scored_dialogs if score > 0.35]
        return filtered_win
    
    def refine_select_win(self, current_query: str, selected_win_content: List[str]) -> List[str]:
        """
        ä½¿ç”¨LLM refineé€‰æ‹©çš„çª—å£å†…å®¹
        
        Args:
            current_query: å½“å‰æŸ¥è¯¢
            selected_win_content: é€‰æ‹©çš„çª—å£å†…å®¹åˆ—è¡¨
            
        Returns:
            ç²¾ç‚¼åçš„çª—å£å†…å®¹åˆ—è¡¨
        """
        if not selected_win_content:
            return [-1]
            
        try:
            # å°è¯•å¤„ç†åµŒå¥—åˆ—è¡¨
            processed_content = []
            for item in selected_win_content:
                if isinstance(item, list):
                    processed_content.append("\n".join(item))
                else:
                    processed_content.append(str(item))
            selected_win_content = processed_content
        except:
            selected_win_content = [str(item) for item in selected_win_content]
        
        conv_info = f"<dialogue_history>:{selected_win_content}\n<current_query>:{current_query},please give the instructions"
        
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = f"{self.prompt_template_refine}\n\nUser: {conv_info}\n\nAssistant:"
        
        for retry_count in range(3):
            try:
                response = self._call_vllm_api(full_prompt)
                result = eval(response)
                return result
            except Exception as e:
                print(f"Error: {e}")
                print(f"Error: Failed to parse response. Retry {retry_count+1}/3")
        
        print("Max retries exceeded.")
        return [-1]
    
    def hierarchical_atten_embedding(self, chunks_list: List[Dict[str, Any]], 
                                   output_path: str, start_idx: int = 0, 
                                   processed_data: List[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        åˆ†å±‚æ³¨æ„åŠ›åµŒå…¥å¤„ç†æ ¸å¿ƒå‡½æ•°
        
        Args:
            chunks_list: æ•°æ®å—åˆ—è¡¨
            output_path: è¾“å‡ºè·¯å¾„
            start_idx: å¼€å§‹ç´¢å¼•
            processed_data: å·²å¤„ç†çš„æ•°æ®
            
        Returns:
            å¤„ç†åçš„æ•°æ®å—åˆ—è¡¨
        """
        if processed_data is None:
            processed_data = []
            
        for i in tqdm(range(start_idx, len(chunks_list)), desc="Processing chunks", unit="chunk"):
            # è·å–å¯¹è¯å†å²
            dialogue_history_list = [chunks_list[j]["content"] for j in range(max(0, i - self.config["max_win"]), i)]
            current_query = chunks_list[i]["content"]
            
            # ä½¿ç”¨LLMé€‰æ‹©çª—å£ï¼ˆå¤šæ¬¡æ¨ç†æŠ•ç¥¨ï¼‰
            win_index, confidence = self.dynamic_win_llm(dialogue_history_list, current_query)
            
            if win_index[0] == -1 or len(win_index) == 0:
                h_length = 0
                selected_win_content = [""]
            else:
                selected_win_index = win_index[0]
                h_length = len(dialogue_history_list) - selected_win_index
                selected_win_content = [chunks_list[j]["content"] for j in range(max(0, i - h_length), i)]
                
                # ä½¿ç”¨rerankerè¿‡æ»¤
                # selected_win_content = self.filter_win_rerank(current_query, selected_win_content)
                
                # ä½¿ç”¨LLM refine
                # selected_win_content = self.refine_select_win(current_query, selected_win_content)
            
            # æ„å»ºåˆ†å±‚å†…å®¹
            if selected_win_content and selected_win_content[0] != -1:
                chunks_list[i]["hierarchical_content"] = "\n".join(selected_win_content) + " | [context] | " + chunks_list[i]["content"]
            else:
                chunks_list[i]["hierarchical_content"] = chunks_list[i]["content"]
            
            chunks_list[i]["h_length"] = h_length
            chunks_list[i]["confidence"] = confidence  # ä¿å­˜ç½®ä¿¡åº¦ä¿¡æ¯
            
            # å®šæœŸä¿å­˜å·²å¤„ç†çš„æ•°æ®
            if i % 10 == 0:
                with open(output_path, "w", encoding="utf-8") as f:
                    result_data = processed_data + chunks_list[start_idx:i + 1]
                    json.dump(result_data, f, ensure_ascii=True, indent=4)
        
        return chunks_list
    
    def test_vllm_determinism(self, test_prompt: str, test_times: int = 10):
        """æµ‹è¯•vLLMçš„ç¡®å®šæ€§"""
        results = []
        for i in range(test_times):
            response = self._call_vllm_api(test_prompt, temperature=0.0)
            results.append(response)
            print(f"ç¬¬{i+1}æ¬¡ç»“æœ: {response}")
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸€è‡´
        unique_results = set(results)
        print(f"å…±äº§ç”Ÿäº† {len(unique_results)} ç§ä¸åŒçš„ç»“æœ")
        if len(unique_results) == 1:
            print("âœ… ç»“æœå®Œå…¨ä¸€è‡´")
        else:
            print("âš ï¸ ç»“æœå­˜åœ¨å·®å¼‚ï¼Œå»ºè®®ä¿ç•™æŠ•ç¥¨æœºåˆ¶")
    
    def process_files(self):
        """
        å¤„ç†æ‰€æœ‰æ–‡ä»¶çš„ä¸»å‡½æ•°
        """
        for file in os.listdir(self.config["data_dir"]):
            if file.endswith(".json"):
                data_path = os.path.join(self.config["data_dir"], file)
                output_data_path = os.path.join(self.config["output_dir"], "dynamic_" + file)
                
                # æ–­ç‚¹ç»­å­˜ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰éƒ¨åˆ†å¤„ç†æ•°æ®
                if os.path.exists(output_data_path):
                    with open(output_data_path, "r", encoding="utf-8") as f:
                        try:
                            processed_data = json.load(f)
                            last_processed_idx = len(processed_data)
                        except json.JSONDecodeError:
                            processed_data = []
                            last_processed_idx = 0
                else:
                    processed_data = []
                    last_processed_idx = 0
                
                with open(data_path, "r", encoding="utf-8") as f:
                    chunks_list = json.load(f)
                
                print(f"ğŸ”„ æ–­ç‚¹ç»­å­˜ï¼šä»ç´¢å¼• {last_processed_idx} ç»§ç»­å¤„ç†...")
                hia_data = self.hierarchical_atten_embedding(chunks_list, output_data_path, 
                                                           start_idx=last_processed_idx, 
                                                           processed_data=processed_data)
                
                result_data = processed_data + hia_data[last_processed_idx:]
                
                # å¤„ç†å®Œæˆåæœ€ç»ˆä¿å­˜
                with open(output_data_path, "w", encoding="utf-8") as f:
                    json.dump(result_data, f, ensure_ascii=True, indent=4)
                
                print("âœ… å¤„ç†å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜ï¼")
    

if __name__ == "__main__":
    config = {
        "data_dir": "./data",
        "output_dir": "./result",
        "prompt_file": "./data/prompt.txt",
        "prompt_file_refine": "./data/prompt_2.txt",
        "rerank_model": "/share/shared_models/models--BAAI--bge-reranker-v2-m3/snapshots/12e974610ba9083ed95f3edf08d7e899581f4de4",
        "max_win": 15
    }
    
    processor = DynamicWindowProcessor(config)
    processor.test_vllm_determinism("è¯·é—®ä½ æ˜¯è°ï¼Ÿ", test_times=5)